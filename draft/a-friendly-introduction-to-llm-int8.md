# A Friendly Introduction to LLM.int8()

目前大模型权重通常以`FP16`或`BF16`格式存储，但是许多研究发现模型推理时可以使用更少的比特位的数据类型，模型可以比较正常运行的同时，还能够大量节省GPU/NPU内存。

* **性能**：本文中将会使用性能一词来代表推理的速度，比如平均输出每个token的耗时，耗时越低意味着性能越强；
* **下游任务精度**：



