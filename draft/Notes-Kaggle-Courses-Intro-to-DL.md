---
title: "Notes: Kaggle Courses - Intro to DL & Computer Vision"
date: 2022-03-12 16:23:00
category: "Notes"
math: true
tags: ["Notes", "Kaggle", "DeepLearning", "Computer Vision"]
---

# Intro to Deep Learning

## A Single Neuron

### The Linear Unit

ä¸‹é¢æ˜¯ä¸€ä¸ª**neuron**ï¼ˆæˆ–ç§°**unit**ï¼‰çš„ç¤ºæ„å›¾ï¼Œ`x`æ˜¯è¾“å…¥ï¼›`w`æ˜¯xçš„æƒé‡**weight**ï¼›`b`æ˜¯**bias**ï¼Œæ˜¯ä¸€ç§ç‰¹æ®Šçš„æƒé‡ï¼Œæ²¡æœ‰å’Œbiasç›¸å…³çš„è¾“å…¥æ•°æ®ï¼Œå®ƒå¯ä»¥ç‹¬ç«‹äºè¾“å…¥ä¿®æ”¹è¾“å‡ºã€‚ç¥ç»ç½‘ç»œé€šè¿‡ä¿®æ”¹æƒé‡æ¥â€œlearnâ€ã€‚

`y`æ˜¯è¿™ä¸ªç¥ç»å…ƒè¾“å‡ºçš„å€¼ï¼Œ$y=wx+b$ï¼Œåˆšå¥½æ˜¯ä¸€ä¸ªç›´çº¿çš„æ–¹ç¨‹ï¼Œwæ˜¯æ–œç‡ï¼Œbæ˜¯åœ¨yè½´ä¸Šçš„æˆªè·ã€‚

![The Linear Unit: ğ‘¦=ğ‘¤ğ‘¥+ğ‘](/blog/Notes-Kaggle-Courses-Intro-to-DL/001.png)


### Example - The Linear Unit as a Model

å•ä¸ªç¥ç»å…ƒæ˜¯é€šå¸¸åªä¼šåœ¨æ›´å¤§çš„ç½‘ç»œä¸­å‘æŒ¥ä½œç”¨ï¼Œå•ç¥ç»å…ƒæ¨¡å‹æ˜¯çº¿æ€§æ¨¡å‹ã€‚å½“$w=2.5, b=90$æ—¶ï¼Œè¿™ä¸ªçº¿æ€§æ¨¡å‹å¯ä»¥ç”¨æ¥ååº”ç³–`'sugars'`å’Œå¡è·¯é‡Œ`'calories'`çš„å…³ç³»ï¼š

![*Computing with the linear unit.*](/blog/Notes-Kaggle-Courses-Intro-to-DL/002.png)

### Multiple Inputs

å¯¹äºå¤šä¸ªè¾“å…¥ï¼Œä¹Ÿæ˜¯è¿™æ ·å°†æ¯ä¸ªè¾“å…¥ä¹˜ä»¥æƒé‡ï¼Œå¹¶æŠŠå®ƒä»¬ç›¸åŠ ã€‚ä¸‹é¢è¿™ä¸ªå¯¹åº”çš„å…¬å¼ä¸ºï¼š$y=w_{0} x_{0}+w_{1} x_{1}+w_{2} x_{2}+b.$
![A linear unit with three inputs.](/blog/Notes-Kaggle-Courses-Intro-to-DL/003.png)

### Linear Units in Keras

åœ¨Kerasä¸­åˆ›å»ºæ¨¡å‹æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨`keras.Sequential`ï¼Œä¸‹é¢è¿™ä¸ªç¤ºä¾‹è¡¨ç¤ºä¸€ä¸ªçº¿æ€§æ¨¡å‹ï¼Œå¯ä»¥è¾“å…¥3ä¸ªç‰¹å¾ï¼ˆ'sugars', 'fiber', 'protein'ï¼‰ï¼Œå¹¶ä¸”åªæœ‰ä¸€ä¸ªè¾“å‡ºï¼š'calories'ã€‚

```python
from tensorflow import keras
from tensorflow.keras import layers

# Create a network with 1 linear unit
model = keras.Sequential([
    layers.Dense(units=1, input_shape=[3])
])
```
ç¬¬ä¸€ä¸ªå‚æ•°unitså®šä¹‰è¾“å‡ºçš„ä¸ªæ•°ï¼Œinput_shapeå‘Šè¯‰Kerasè¾“å…¥ç‰¹å¾çš„æ•°é‡ã€‚ç›®å‰åªéœ€è¦ç”¨åˆ°`input_shape=[num_columns]`ï¼Œinput_shapeè¿˜å¯ä»¥æ”¯æŒä½¿ç”¨æ›´å¤æ‚çš„æ•°æ®ï¼š`[height, width, channels]`ã€‚

`Tensors`æ˜¯TensorFlowç‰ˆæœ¬çš„numpyæ•°ç»„ï¼Œå¹¶ä¸”åšäº†ä¸€äº›ä½¿å®ƒæ›´é€‚åˆç”¨äºæœºå™¨å­¦ä¹ çš„æ”¹å˜ï¼ŒTensorsä¸GPU/TPUåŠ é€Ÿå™¨å…¼å®¹ï¼Œè€ŒTPUå°±æ˜¯ä¸“ä¸ºTensorsè€Œè®¾è®¡çš„ã€‚åœ¨Keraså†…éƒ¨ï¼Œä½¿ç”¨Tensorsè¡¨ç¤ºç¥ç»ç½‘ç»œçš„æƒé‡ã€‚

`model.weights`å¯ä»¥ç”¨æ¥æŸ¥çœ‹æƒé‡ï¼Œåœ¨è®­ç»ƒå¼€å§‹å‰ï¼Œæƒé‡éƒ½ä¼šè¢«åˆå§‹åŒ–ä¸ºéšæœºå€¼ã€‚

## Deep Neural Networks

### Layers

ç¥ç»ç½‘ç»œä¼šå°†ç¥ç»å…ƒç»„æˆ**å±‚ï¼ˆlayersï¼‰**ï¼Œåˆå¹¶æœ‰ç›¸åŒçš„è¾“å…¥çš„çº¿æ€§ç¥ç»å…ƒï¼Œå°±å¾—åˆ°äº†ä¸€ä¸ª**ç¨ å¯†å±‚ï¼ˆdense layerï¼‰**
![A dense layer of two linear units receiving two inputs and a bias.](/blog/Notes-Kaggle-Courses-Intro-to-DL/004.png)

### The Activation Function

ä¸¤ä¸ªä¸­é—´æ²¡æœ‰å…¶ä»–ä¸œè¥¿çš„ç¨ å¯†å±‚ï¼Œæ•ˆæœå¹¶ä¸ä¼šæ¯”ä¸€ä¸ªç¨ å¯†å±‚çš„æ•ˆæœå¥½å¤šå°‘ï¼Œâ€œç¨ å¯†å±‚æœ¬èº«ä¸èƒ½å¸¦æˆ‘ä»¬ç¦»å¼€çº¿å’Œé¢çš„ä¸–ç•Œâ€ï¼Œæˆ‘ä»¬éœ€è¦çš„æ˜¯éçº¿æ€§ï¼ˆnonlinearï¼‰ï¼Œéœ€è¦æ¿€æ´»å‡½æ•°ï¼ˆactivation functionï¼‰ã€‚

![æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œæ¨¡å‹åªèƒ½å­¦ä¹ çº¿æ€§å…³ç³»ï¼Œä¸ºäº†æ‹Ÿåˆæ›²çº¿ï¼Œéœ€è¦ä½¿ç”¨æ¿€æ´»å‡½æ•°](/blog/Notes-Kaggle-Courses-Intro-to-DL/005.png)

æ¿€æ´»å‡½æ•°å°±æ˜¯åº”ç”¨äºæ¯ä¸€å±‚è¾“å‡ºçš„å‡½æ•°ï¼Œæœ€å¸¸è§çš„æ˜¯*rectifier*å‡½æ•°$max(0,x).$

![image](/blog/Notes-Kaggle-Courses-Intro-to-DL/006.png)

æŠŠrectifieråº”ç”¨åˆ°ä¸€ä¸ªçº¿æ€§å•å…ƒä¸Šæ—¶ï¼Œå°±å¾—åˆ°äº†**rectified linear unit**ï¼Œæˆ–ç®€ç§°**ReLU**ã€‚è¿™æ ·è¿™ä¸ªçº¿æ€§å•å…ƒçš„è¾“å‡ºå°±æ˜¯$max(0,w\cdot x+b)$

![image](/blog/Notes-Kaggle-Courses-Intro-to-DL/007.png)

### Stacking Dense Layers

å †å å±‚æ¥è·å¾—å¤æ‚çš„æ•°æ®è½¬æ¢ï¼š
![A stack of dense layers makes a "fully-connected" network.](/blog/Notes-Kaggle-Courses-Intro-to-DL/008.png)

è¾“å‡ºå±‚ä¹‹å‰çš„å±‚æœ‰æ—¶è¢«ç§°ä¸º**éšè—å±‚ï¼ˆhiddenï¼‰**ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰ç›´æ¥çœ‹åˆ°å®ƒä»¬çš„è¾“å‡ºã€‚ä¸Šå›¾åœ¨è¾“å‡ºä¹‹å‰ä½¿ç”¨äº†ä¸€ä¸ªçº¿æ€§å•å…ƒï¼Œè€Œä¸æ˜¯æ¿€æ´»å‡½æ•°ï¼Œè¿™æ ·åšä½¿è¿™ä¸ªæ¨¡å‹é€‚ç”¨äºå›å½’ä»»åŠ¡ï¼Œåœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œå¯èƒ½è¦åœ¨è¿™é‡Œä½¿ç”¨æ¿€æ´»å‡½æ•°ã€‚

#### Building Sequential Models

æˆ‘ä»¬å°†ç”¨`Sequential`æ¨¡å‹æ¥è¿æ¥ä¸€ç³»åˆ—çš„å±‚ï¼Œå»ºç«‹ä¸Šå›¾çš„æ¨¡å‹ï¼Œç¬¬ä¸€æ¬¡è·å¾—è¾“å…¥ï¼Œæœ€åä¸€å±‚äº§ç”Ÿè¾“å‡º
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    # ReLU éšè—å±‚
    layers.Dense(units=4, activation='relu', input_shape=[2]), # è¾“å…¥
    layers.Dense(units=3, activation='relu'),
    # çº¿æ€§è¾“å‡ºå±‚
    layers.Dense(units=1),
])
```
ä¸€å®šè¦æŠŠæ‰€æœ‰çš„å›¾å±‚æ”¾åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œæ¯”å¦‚`[layerï¼Œlayerï¼Œlayerï¼Œ...]`ã€‚è¦æ·»åŠ æ¿€æ´»å‡½æ•°å±‚ï¼Œåªè¦è®¾ç½®`activation`å‚æ•°å³å¯ï¼Œæ¯”å¦‚ReLUï¼š`activation='relu'`


## Stochastic Gradient Descent

### Introduction

å‰é¢ä¸¤èŠ‚è®²äº†å¦‚ä½•æ„å»ºå…¨è¿æ¥çš„ç½‘ç»œï¼ˆfully-connected networksï¼‰ï¼Œæ–°åˆ›å»ºçš„ç½‘ç»œä¸­çš„æƒé‡éƒ½æ˜¯éšæœºçš„ï¼Œè¿™ä¸€èŠ‚å°±å¼€å§‹ä»‹ç»å¦‚ä½•è®­ç»ƒç¥ç»ç½‘ç»œã€‚

è®­ç»ƒæ¨¡å‹ä¸­æ¯æ¡æ•°æ®éœ€è¦è¾“å…¥ä¸€äº›ç‰¹å¾ï¼ˆfeaturesï¼‰å’Œä¸€ä¸ªæœŸæœ›çš„è¾“å‡ºç›®æ ‡ï¼ˆtargetï¼‰ï¼Œè®­ç»ƒçš„è¿‡ç¨‹ä¼šè°ƒæ•´æƒé‡ï¼Œä½¿ç½‘ç»œå¯ä»¥é€šè¿‡è¾“å…¥çš„ç‰¹å¾è®¡ç®—å‡ºæœŸæœ›çš„ç›®æ ‡ã€‚

é™¤äº†è®­ç»ƒæ•°æ®ï¼Œè¿˜éœ€è¦ï¼š
- â€œæŸå¤±å‡½æ•°â€ï¼Œç”¨æ¥è¡¡é‡ç½‘ç»œé¢„æµ‹ç»“æœçš„å¥½åã€‚
- â€œä¼˜åŒ–å™¨â€ï¼Œå¯ä»¥å‘Šè¯‰ç½‘ç»œå¦‚ä½•æ”¹å˜å…¶æƒé‡ã€‚

### The Loss Function

**æŸå¤±å‡½æ•°**ï¼ˆloss functionï¼‰æµ‹é‡targetçœŸå®å€¼å’Œæ¨¡å‹é¢„æµ‹å€¼ä¹‹é—´çš„å·®å¼‚ã€‚ä¸åŒçš„é—®é¢˜éœ€è¦ä½¿ç”¨ä¸åŒçš„æŸå¤±å‡½æ•°ï¼Œæ¯”å¦‚**å›å½’é—®é¢˜**ï¼ˆregression problemsï¼‰å¸¸ç”¨çš„æŸå¤±å‡½æ•°å°±æ˜¯å¹³å‡ç»å¯¹è¯¯å·®**MAE**ï¼ˆmean absolute errorï¼‰ï¼ŒMAEé€šè¿‡å·®çš„ç»å¯¹å€¼`abs(y_true-y_pred)`æµ‹é‡é¢„æµ‹å€¼`y_pred`ä¸çœŸå®ç›®æ ‡`y_true`çš„å·®å¼‚ã€‚
æ•°æ®é›†ä¸Šçš„æ€»MAEï¼Œæ˜¯æ‰€æœ‰è¿™äº›å·®çš„ç»å¯¹å€¼çš„å¹³å‡å€¼ã€‚

![å¹³å‡ç»å¯¹è¯¯å·®æ˜¯æ‹Ÿåˆæ›²çº¿å’Œæ•°æ®ç‚¹ä¹‹é—´çš„å¹³å‡é•¿åº¦](/blog/Notes-Kaggle-Courses-Intro-to-DL/009.png)

é™¤äº†MAEä¹‹å¤–ï¼Œå›å½’é—®é¢˜è¿˜æœ‰å…¶ä»–çš„æŸå¤±å‡½æ•°ï¼šå‡æ–¹è¯¯å·®ï¼ˆmean-squared errorï¼ŒMSEï¼‰æˆ–HuberæŸå¤±ï¼ˆHuber lossï¼‰ï¼Œå®ƒä»¬éƒ½å¯ä»¥åœ¨Kerasä¸­ä½¿ç”¨ã€‚
åœ¨è®­ç»ƒæœŸé—´ï¼Œæ¨¡å‹å°†ä½¿ç”¨æŸå¤±å‡½æ•°ä½œä¸ºæŒ‡å¯¼ï¼Œä»¥æ‰¾åˆ°æ­£ç¡®çš„æƒé‡å€¼ï¼ˆlossè¶Šå°è¶Šå¥½ï¼‰ã€‚æ¢å¥è¯è¯´ï¼ŒæŸå¤±å‡½æ•°å‘Šè¯‰ç½‘ç»œå®ƒçš„ç›®æ ‡ã€‚

### The Optimizer - Stochastic Gradient Descent

> éšæœºæ¢¯åº¦ä¸‹é™ï¼Œè¿™é‡Œçš„â€œéšæœºâ€ç”¨çš„æ˜¯stochasticï¼Œè€Œérandomï¼ŒæŸ¥äº†ä¸€ä¸‹ç»´åŸºç™¾ç§‘ï¼š

> Although stochasticity and randomness are distinct in that the former refers to a modeling approach and the latter refers to phenomena themselves, these two terms are often used synonymously. Furthermore, in probability theory, the formal concept of a stochastic process is also referred to as a random process.

> stochastic åå‘æŒ‡å»ºæ¨¡æ–¹æ³•ï¼Œrandom åå‘æŒ‡ç°è±¡æœ¬èº«ï¼Œå¾ˆå¤šæ—¶å€™è¿™ä¸¤ä¸ªè¯æ˜¯åŒä¹‰çš„ã€‚

**ä¼˜åŒ–å™¨**ï¼ˆoptimizerï¼‰æ˜¯ä¸€ç§è°ƒæ•´æƒé‡æ¥ä½¿lossæœ€å°åŒ–çš„ç®—æ³•ã€‚æ·±åº¦å­¦ä¹ ä¸­ä½¿ç”¨çš„æ‰€æœ‰ä¼˜åŒ–å™¨ç®—æ³•éƒ½å±äºä¸€ä¸ªå«åšéšæœºæ¢¯åº¦ä¸‹é™çš„å®¶æ—ï¼Œè®­ç»ƒç½‘ç»œçš„è¿‡ç¨‹å°±æ˜¯ä¸€æ¬¡æ¬¡è¿­ä»£ä¸‹é¢çš„ç®—æ³•ï¼š

1. é‡‡é›†ä¸€äº›è®­ç»ƒæ•°æ®ï¼Œé€šè¿‡ç½‘ç»œè¿›è¡Œé¢„æµ‹
2. æµ‹é‡é¢„æµ‹å€¼å’ŒçœŸå®å€¼ä¹‹é—´çš„æŸå¤±
3. æœ€åï¼Œè°ƒæ•´æƒé‡ä½¿lossæ›´å°

<img src="/blog/Notes-Kaggle-Courses-Intro-to-DL/010.gif">
<p class="image-caption">ä½¿ç”¨éšæœºæ¢¯åº¦ä¸‹é™çš„ç¥ç»ç½‘ç»œ</p>

æ¯ä¸ªè¿­ä»£çš„è®­ç»ƒæ•°æ®æ ·æœ¬ç§°ä¸ºä¸€ä¸ª**minibatch**ï¼ˆæˆ–ç§°**batch**ï¼‰ï¼Œè€Œä¸€è½®å®Œæ•´çš„è®­ç»ƒæ•°æ®ç§°ä¸ºä¸€ä¸ª**epoch**ã€‚ä½ è®­ç»ƒçš„æ¬¡æ•°æ˜¯ç½‘ç»œçœ‹åˆ°æ¯ä¸ªè®­ç»ƒç¤ºä¾‹çš„æ¬¡æ•°ã€‚ç½‘ç»œçœ‹åˆ°æ¯ä¸ªè®­ç»ƒç¤ºä¾‹çš„æ¬¡æ•°ï¼Œå°±æ˜¯è®­ç»ƒçš„è½®æ•°ã€‚

ä¸Šé¢çš„åŠ¨ç”»æ˜¾ç¤ºäº†çº¿æ€§æ¨¡å‹åœ¨ä½¿ç”¨SGDè¿›è¡Œè®­ç»ƒï¼Œæ·¡çº¢è‰²çš„ç‚¹æ˜¯æ•´ä¸ªæ•°æ®é›†ï¼Œå˜åŒ–çš„å®å¿ƒçº¢ç‚¹è¡¨ç¤ºminibatchï¼Œæ¯æ¬¡SGDçœ‹åˆ°ä¸€ä¸ªæ–°çš„minibatchï¼Œå®ƒéƒ½ä¼šå°†æƒé‡ï¼ˆ`w`æ–œç‡ï¼Œ`b`yè½´æˆªè·ï¼‰ç§»å‘batchçš„æ­£ç¡®å€¼ï¼Œç»è¿‡ä¸€è½®åˆä¸€è½®çš„batchï¼Œç›´çº¿æœ€ç»ˆä¼šæ”¶æ•›åˆ°æœ€ä½³çŠ¶æ€ï¼Œå¯ä»¥çœ‹åˆ°ï¼Œæƒé‡è¶Šæ¥è¿‘çœŸå®å€¼ï¼Œlosså°±è¶Šå°ã€‚

### Learning Rate and Batch Size

å¯ä»¥æ³¨æ„åˆ°ç›´çº¿æ¯æ¬¡ä¼šåœ¨batchçš„æ–¹å‘ä¸Šå‘ç”Ÿä¸€ä¸ªå°çš„ç§»åŠ¨ï¼Œè¿™ä¸ªç§»åŠ¨å˜åŒ–çš„å¤§å°å–å†³äº**å­¦ä¹ ç‡**ï¼ˆlearning rateï¼‰ï¼Œå­¦ä¹ ç‡è¶Šå°ï¼Œåœ¨é€¼è¿‘æ­£ç¡®å€¼çš„è¿‡ç¨‹å°±è¶Šé•¿ã€‚

å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰å’Œbatchçš„å¤§å°ï¼ˆBatch Sizeï¼‰æ˜¯å¯¹SGDè®­ç»ƒè¿›åº¦å½±å“æœ€å¤§çš„ä¸¤ä¸ªå‚æ•°ã€‚å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨å¾€å¾€å¾ˆå¾®å¦™ï¼Œå¯¹è¿™äº›å‚æ•°çš„æ­£ç¡®é€‰æ‹©å¹¶ä¸æ€»æ˜¯æ˜¾è€Œæ˜“è§çš„ã€‚ï¼ˆæˆ‘ä»¬å°†åœ¨ç»ƒä¹ ä¸­æ¢è®¨è¿™äº›å½±å“ï¼‰

å¹¸è¿çš„æ˜¯ï¼Œå¯¹äºå¤§å¤šæ•°å·¥ä½œæ¥è¯´ï¼Œæ²¡æœ‰å¿…è¦è¿›è¡Œå¹¿æ³›çš„è¶…å‚æ•°æœç´¢ä»¥è·å¾—æ»¡æ„çš„ç»“æœã€‚Adamæ˜¯ä¸€ç§SGDç®—æ³•ï¼Œå…·æœ‰è‡ªé€‚åº”å­¦ä¹ ç‡ï¼Œä½¿å…¶é€‚ç”¨äºå¤§å¤šæ•°é—®é¢˜ï¼Œè€Œæ— éœ€ä»»ä½•å‚æ•°è°ƒæ•´ï¼ˆä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œå®ƒæ˜¯â€œè‡ªè°ƒæ•´â€ï¼‰ã€‚Adamæ˜¯ä¸€ä¸ªä¼Ÿå¤§çš„é€šç”¨ä¼˜åŒ–å™¨ã€‚

### Adding the Loss and Optimizer

å®šä¹‰æ¨¡å‹åï¼Œå¯ä»¥ä½¿ç”¨æ¨¡å‹çš„`compile`æ–¹æ³•æ·»åŠ æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ï¼š

```python
model.compile(
    optimizer="adam",
    loss="mae",
)
```

åªéœ€è¦ä¸€ä¸ªå­—ç¬¦ä¸²å°±å¯ä»¥æŒ‡å®šlosså’Œoptimizerï¼›ä¹Ÿå¯ä»¥é€šè¿‡Keras APIç›´æ¥è®¿é—®è¿™äº›å‚æ•°â€”â€”ä¾‹å¦‚æƒ³è¦ä¼˜åŒ–å‚æ•°â€”â€”ä½†å¯¹æˆ‘ä»¬æ¥è¯´ï¼Œé»˜è®¤å€¼å°±å¯ä»¥æ­£å¸¸å·¥ä½œã€‚

æ¢¯åº¦ï¼ˆ**gradient**ï¼‰æ˜¯ä¸€ä¸ªå‘é‡ï¼Œå‘Šè¯‰æˆ‘ä»¬æƒé‡åº”è¯¥æœå“ªä¸ªæ–¹å‘ç§»åŠ¨ï¼Œä¹Ÿå°±æ˜¯è¯´å®ƒå‘Šè¯‰æˆ‘ä»¬å¦‚ä½•æ”¹å˜é‡é‡ä½¿æŸå¤±å˜åŒ–æœ€å¿«ï¼Œæˆ‘ä»¬ç§°è¿‡ç¨‹ä¸ºæ¢¯åº¦ä¸‹é™ï¼ˆ**descent**ï¼‰ï¼Œæ˜¯å› ä¸ºå®ƒä½¿ç”¨æ¢¯åº¦å°†æŸå¤±æ›²çº¿ä¸‹é™åˆ°æœ€å°å€¼ï¼Œéšæœºï¼ˆ**stochastic**ï¼‰æ˜¯æŒ‡æ¯æ¬¡é€‰å–çš„minibatchesæ˜¯ä»è®­ç»ƒæ•°æ®ä¸­éšæœºé€‰å–çš„éšæœºæ ·ä¾‹ã€‚SGDå³Stochastic Gradient Descentã€‚

```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=[11]),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(1),
])

# ç»™æ¨¡å‹è®¾ç½®ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
model.compile(
    optimizer='adam',
    loss='mae',
)

# æ¯è½®ç»™æ¨¡å‹ 256 è¡Œæ•°æ®ï¼Œè¿™æ ·è®­ç»ƒ 10 è½®
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=10,
)
```

åœ¨æ¯ä¸€è½®çš„è®­ç»ƒåï¼Œéƒ½ä¼šè¾“å‡ºå½“å‰çš„lossï¼Œå¹¶ä¸”è®­ç»ƒè¿‡ç¨‹ä¸­çš„losséƒ½ä¼šè¢«ä¿å­˜èµ·æ¥ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç”¨å®ƒä»¬ä½œå›¾æ¥æ›´ç›´è§‚çš„çœ‹å‡ºlossçš„å˜åŒ–ï¼š

```python
import pandas as pd

# convert the training history to a dataframe
history_df = pd.DataFrame(history.history)
# use Pandas native plot method
history_df['loss'].plot()
```

![](/blog/Notes-Kaggle-Courses-Intro-to-DL/011.png)

lossçš„å˜åŒ–æ›²çº¿é€æ¸å˜å¾—è¶‹äºæ°´å¹³äº†ï¼Œå°±è¯´æ˜æ¨¡å‹å·²ç»å­¦ä¼šäº†å®ƒèƒ½å­¦ä¼šçš„ä¸€åˆ‡ï¼Œæ‰€ä»¥æ²¡æœ‰å¿…è¦è®©å®ƒè¿›è¡Œæ›´å¤šçš„è¿­ä»£ï¼Œå¦‚æœæƒ³è¦ä¼˜åŒ–lossï¼Œæ›´åº”è¯¥åšçš„æ˜¯è°ƒæ•´æ¨¡å‹ã€‚

## Overfitting and Underfitting

### Interpreting the Learning Curves

æ¨¡å‹å¾—åˆ°çš„æ•°æ®æ˜¯ç”±**ä¿¡æ¯**ï¼ˆsignalï¼‰å’Œ**å™ªå£°**ï¼ˆnoiseï¼‰ç»„æˆçš„ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒä»signalä¸­å­¦ä¹ åˆ°æ¨¡å¼ï¼Œè¿™æ ·å¯ä»¥ä½¿å…¶åœ¨é¢„æµ‹è¿‡ç¨‹ä¸­è¡¨ç°è‰¯å¥½ï¼Œå™ªå£°æ˜¯åªåœ¨è®­ç»ƒæ•°æ®ä¸­æ­£ç¡®çš„æ¡ˆä¾‹ã€‚

ä¸‹å›¾ç»˜åˆ¶äº†åœ¨è®­ç»ƒæ•°æ®ä¸Šå’Œåœ¨æµ‹è¯•æ•°æ®ä¸Šçš„lossæƒ…å†µï¼Œè¿™äº›æ›²çº¿æˆ‘ä»¬ç§°ä¹‹ä¸º**å­¦ä¹ æ›²çº¿**ï¼ˆlearning curvesï¼‰ï¼Œä¸ºäº†æœ‰æ•ˆçš„è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæˆ‘ä»¬éœ€è¦èƒ½å¤Ÿè§£é‡Šå®ƒä»¬ã€‚

![The validation loss gives an estimate of the expected error on unseen data.](/blog/Notes-Kaggle-Courses-Intro-to-DL/012.png)

åœ¨æ¨¡å‹å­¦ä¹ signalå’Œnoiseçš„è¿‡ç¨‹ä¸­ï¼Œè®­ç»ƒlossä¼šé€æ¸ä¸‹é™ï¼Œä½†åªæœ‰æ¨¡å‹å­¦ä¹ åˆ°signalæ—¶ï¼ŒéªŒè¯lossæ‰ä¼šé™ä½ã€‚åœ¨å­¦ä¹ signalçš„è¿‡ç¨‹ä¸­ï¼Œä¸¤æ¡æ›²çº¿éƒ½ä¼šä¸‹é™ï¼Œä½†æ˜¯å¦‚æœæ¨¡å‹å­¦ä¹ äº†noiseï¼Œé‚£ä¹ˆä¸¤æ¡æ›²çº¿ä¹‹é—´å°±ä¼šå‡ºç°ç©ºéš™ï¼ˆgapï¼‰ï¼Œè¿™ä¸ªç©ºéš™çš„å¤§å°ï¼Œå¯ä»¥ååº”æ¨¡å‹å­¦åˆ°äº†å¤šå°‘noiseã€‚ç†æƒ³æƒ…å†µä¸‹æˆ‘ä»¬å¸Œæœ›æ¨¡å‹åªå­¦ä¹ signalä¸å­¦ä¹ noiseï¼Œä½†è¿™å‡ ä¹æ˜¯ä¸å¯èƒ½çš„ï¼Œåªèƒ½ä»¥å­¦ä¹ åˆ°å¾ˆå¤šçš„noiseä¸ºä»£ä»·ï¼Œè®©æ¨¡å‹å°½å¯èƒ½å¤šçš„å­¦ä¹ åˆ°signalï¼Œä»ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼Œå½“å‡ºç°äº†æŸä¸€ç‚¹åï¼ŒéªŒè¯lossä¼šé€æ¸ä¸Šå‡ã€‚

![Underfitting and overfitting.](/blog/Notes-Kaggle-Courses-Intro-to-DL/013.png)


åœ¨è®­ç»ƒæ¨¡å‹æ—¶å¯èƒ½ä¼šå‡ºç°ä¸¤ä¸ªé—®é¢˜ï¼š
1. signalä¸è¶³æˆ–å™ªå£°è¿‡å¤§ã€‚æœªå……åˆ†æ‹Ÿåˆè®­ç»ƒé›† æ˜¯æŒ‡ç”±äºæ¨¡å‹æ²¡æœ‰å­¦ä¹ åˆ°è¶³å¤Ÿçš„signalï¼Œå¯¼è‡´lossæ²¡æœ‰å°½å¯èƒ½ä½ã€‚
2. è¿‡åº¦æ‹Ÿåˆè®­ç»ƒé›†æ˜¯æŒ‡ç”±äºæ¨¡å‹å­¦ä¹ äº†å¤ªå¤šçš„å™ªå£°ï¼Œå¯¼è‡´lossæ²¡æœ‰å°½å¯èƒ½ä½ã€‚

è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è¯€çªæ˜¯åœ¨ä¸¤è€…ä¹‹é—´æ‰¾åˆ°æœ€ä½³å¹³è¡¡ï¼Œæˆ‘ä»¬å°†ç ”ç©¶å‡ ç§ä»è®­ç»ƒæ•°æ®ä¸­è·å–æ›´å¤šsignalçš„æ–¹æ³•ï¼ŒåŒæ—¶å‡å°‘å™ªå£°ã€‚

### Capacity

æ¨¡å‹çš„å®¹é‡æ˜¯æŒ‡å®ƒèƒ½å¤Ÿå­¦ä¹ çš„æ¨¡å¼çš„å¤§å°å’Œå¤æ‚æ€§ï¼Œå¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼Œè¿™åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºå®ƒæœ‰å¤šå°‘ç¥ç»å…ƒä»¥åŠå®ƒä»¬å¦‚ä½•è¿æ¥åœ¨ä¸€èµ·ã€‚å¦‚æœç½‘ç»œä¼¼ä¹ä¸é€‚åˆæ•°æ®ï¼Œåº”è¯¥å°è¯•å¢åŠ å…¶å®¹é‡ã€‚

å¯ä»¥é€šè¿‡ä½¿ç½‘ç»œæ›´å®½ï¼ˆå°†æ›´å¤šç¥ç»å…ƒæ·»åŠ åˆ°ç°æœ‰å±‚ï¼‰æˆ–ä½¿å…¶æ›´æ·±ï¼ˆæ·»åŠ æ›´å¤šå±‚ï¼‰æ¥å¢åŠ ç½‘ç»œçš„å®¹é‡ã€‚æ›´å®½çš„ç½‘ç»œæ›´å®¹æ˜“å­¦ä¹ æ›´å¤šçš„çº¿æ€§å…³ç³»ï¼Œè€Œæ›´æ·±çš„ç½‘ç»œæ›´å–œæ¬¢éçº¿æ€§å…³ç³»ã€‚å“ªä¸ªæ›´å¥½å–å†³äºæ•°æ®é›†ã€‚

```python
model = keras.Sequential([
    layers.Dense(16, activation='relu'),
    layers.Dense(1),
])

wider = keras.Sequential([
    layers.Dense(32, activation='relu'),
    layers.Dense(1),
])

deeper = keras.Sequential([
    layers.Dense(16, activation='relu'),
    layers.Dense(16, activation='relu'),
    layers.Dense(1),
])
```

### Early Stopping

å½“æ¨¡å‹å­¦ä¹ åˆ°å¾ˆå¤šå™ªå£°æ—¶ï¼ŒéªŒè¯æŸå¤±å¯èƒ½ä¼šåœ¨è®­ç»ƒæœŸé—´å¼€å§‹å¢åŠ ï¼Œä¸ºäº†é˜²æ­¢è¿™ç§æƒ…å†µï¼Œåªè¦éªŒè¯lossä¼¼ä¹ä¸å†å‡å°‘ï¼Œæˆ‘ä»¬å°±å¯ä»¥åœæ­¢è®­ç»ƒã€‚ä»¥è¿™ç§æ–¹å¼ä¸­æ–­è®­ç»ƒè¢«ç§°ä¸º**æå‰åœæ­¢**ï¼ˆEarly Stoppingï¼‰ã€‚

![è®©æ¨¡å‹åœ¨éªŒè¯lossæœ€å°çš„ä½ç½®åœæ­¢](/blog/Notes-Kaggle-Courses-Intro-to-DL/014.png)

ä¸€æ—¦æ£€æµ‹åˆ°äº†éªŒè¯losså†æ¬¡ä¸Šå‡ï¼Œå°±å¯ä»¥å°†æƒé‡é‡ç½®å›ä¹‹å‰lossæœ€å°å€¼çš„ä½ç½®ï¼Œç¡®ä¿äº†æ¨¡å‹ä¸ä¼šç»§ç»­è¿‡æ‹Ÿåˆã€‚

### Adding Early Stopping

åœ¨Kerasä¸­ï¼Œæˆ‘ä»¬é€šè¿‡**å›è°ƒ**ï¼ˆcallbackï¼‰åœ¨è®­ç»ƒä¸­æå‰åœæ­¢ï¼Œå›è°ƒå‡½æ•°æ˜¯ä¸€ä¸ªåœ¨ç½‘ç»œè¿è¡Œæ—¶éœ€è¦ç»å¸¸è¿è¡Œçš„å‡½æ•°ã€‚æå‰åœæ­¢å›è°ƒå°†åœ¨æ¯ä¸ªepochä¹‹åè¿è¡Œã€‚ï¼ˆKerasé¢„å…ˆå®šä¹‰äº†å„ç§æœ‰ç”¨çš„å›è°ƒï¼Œä¹Ÿå¯ä»¥å®šä¹‰è‡ªå·±çš„å›è°ƒã€‚ï¼‰

```python
from tensorflow.keras.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # 20ä¸ª
    restore_best_weights=True,
)
```
å¦‚æœåœ¨è¿‡å»20ä¸ªepochsä¸­ï¼ŒéªŒè¯lossæ²¡æœ‰æ”¹å–„0.001ï¼Œé‚£ä¹ˆåœæ­¢è®­ç»ƒï¼Œä¿ç•™æ‰¾åˆ°çš„æœ€ä½³æ¨¡å‹ã€‚


### Example - Train a Model with Early Stopping

```python
from tensorflow import keras
from tensorflow.keras import layers, callbacks

early_stopping = callbacks.EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)

model = keras.Sequential([
    layers.Dense(512, activation='relu', input_shape=[11]),
    layers.Dense(512, activation='relu'),
    layers.Dense(512, activation='relu'),
    layers.Dense(1),
])
model.compile(
    optimizer='adam',
    loss='mae',
)
history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=500,
    callbacks=[early_stopping], # put your callbacks in a list
    verbose=0,  # turn off training log
)

history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
print("Minimum validation loss: {}".format(history_df['val_loss'].min()))
```

## Dropout and Batch Normalization

kerasæœ‰å‡ åä¸­layersï¼Œå¯ä»¥åœ¨ [Keras docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers/) ä¸­æŸ¥çœ‹ç¤ºä¾‹ã€‚è¿™ä¸€èŠ‚å°†ä»‹ç»ä¸¤ç§ç‰¹æ®Šçš„layerï¼Œå®ƒä»¬æœ¬èº«ä¸åŒ…å«ç¥ç»å…ƒã€‚

### Dropout

Dropoutå¯ä»¥å¸®åŠ©ä¿®æ­£overfittingã€‚

å‰é¢è®²è¿‡äº†æ•°æ®çš„æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆï¼Œå‡ºç°äº†è¿‡æ‹Ÿåˆçš„æƒ…å†µï¼Œæ˜¯å› ä¸ºç½‘ç»œæ¨¡å‹å­¦ä¹ äº†è®­ç»ƒæ•°æ®ä¸­çš„è™šå‡çš„æ¨¡å¼ï¼ˆå™ªå£°çš„æ¨¡å¼ï¼‰ï¼Œæ¨¡å‹ä¸ºäº†å­¦ä¹ åˆ°è¿™ä¸ªæ¨¡å¼ é€šå¸¸ä¼šä¾èµ–éå¸¸ç‰¹å®šçš„æƒé‡ç»„åˆï¼Œæ‰€ä»¥è¿™ç§æƒé‡çš„ç»„åˆæ‰å®ç°å‡ºçš„æ¨¡å¼ï¼Œå¾€å¾€æ˜¯å¾ˆè„†å¼±çš„ï¼Œç§»é™¤ä¸€ä¸ªå°±ä¼šç“¦è§£ã€‚

Dropoutå°±æ˜¯åœ¨è®­ç»ƒçš„æ¯ä¸€æ­¥éƒ½éšæœºåˆ é™¤ä¸€å±‚è¾“å…¥å•å…ƒçš„ä¸€å°éƒ¨åˆ†ï¼Œè¿™ä½¿å¾—ç½‘ç»œæ›´éš¾ä»è®­ç»ƒæ•°æ®ä¸­å­¦ä¹ è¿™äº›è™šå‡æ¨¡å¼ã€‚ç›¸åï¼Œå®ƒå¿…é¡»å¯»æ‰¾å¹¿æ³›çš„ã€ä¸€èˆ¬çš„æ¨¡å¼ï¼Œå…¶æƒé‡æ¨¡å¼å¾€å¾€æ›´ç¨³å¥ã€‚

![Here, 50% dropout has been added between the two hidden layers.](/blog/Notes-Kaggle-Courses-Intro-to-DL/015.gif)

### Adding Dropout

åœ¨Kerasä¸­ï¼Œdropoutç‡å‚æ•°`rate`å®šä¹‰äº†è¦å…³é—­çš„è¾“å…¥å•å…ƒçš„ç™¾åˆ†æ¯”ã€‚å°†`Dropout layer`æ”¾åœ¨è¦åº”ç”¨Dropoutçš„å±‚ä¹‹å‰ï¼š
```python
keras.Sequential([
    # ...
    layers.Dropout(rate=0.3), # apply 30% dropout to the next layer
    layers.Dense(16),
    # ...
])
```

### Batch Normalization

"batch normalization"æˆ–ç§°"batchnorm"è¿™ä¸ªç‰¹æ®Šå±‚æœ‰åŠ©äºçº æ­£ç¼“æ…¢æˆ–ä¸ç¨³å®šçš„è®­ç»ƒã€‚åœ¨ç¥ç»ç½‘ç»œä¸­é€šå¸¸éœ€è¦å°†æ‰€æœ‰çš„æ•°æ®æ”¾åœ¨ä¸€ä¸ªé€šç”¨çš„å°ºåº¦ä¸Šï¼Œå¯ä»¥ä½¿ç”¨scikit learnçš„StandardScaleræˆ–MinMaxScalerä¹‹ç±»çš„å·¥å…·ï¼Œè¿™æ˜¯å› ä¸ºSGDæ ¹æ®æ•°æ®äº§ç”Ÿçš„æ¿€æ´»é‡æŒ‰æ¯”ä¾‹æ”¹å˜ç½‘ç»œä¸­çš„æƒé‡ï¼Œè®­ç»ƒä¸­çš„æ•°å€¼å¤§å°èŒƒå›´ä¸ä¸€æ ·å¯èƒ½ä¼šå¯¼è‡´ä¸ç¨³å®šçš„è®­ç»ƒã€‚

å¯ä»¥åœ¨æ•°æ®è¿›å…¥ç½‘ç»œä¹‹å‰å¯¹å…¶è¿›è¡Œè§„èŒƒåŒ–ï¼ˆnormalizeï¼‰ï¼Œä½†æ˜¯æ›´å¥½çš„æ“ä½œæ˜¯åœ¨ç½‘ç»œçš„å†…éƒ¨å¯¹æ•°æ®è¿›è¡Œè§„èŒƒåŒ–ï¼Œ**batch normalization layer**å°±æ˜¯ç”¨æ¥å¯¹ç½‘ç»œä¸­çš„æ•°æ®è¿›è¡Œè§„èŒƒåŒ–æ“ä½œçš„ï¼Œå®ƒä¼šç”¨å…¶è‡ªèº«çš„**å¹³å‡å€¼**å’Œ**æ ‡å‡†å·®**å¯¹batchè¿›è¡Œæ ‡å‡†åŒ–ï¼Œç„¶åç”¨ä¸¤ä¸ªå¯è®­ç»ƒçš„é‡ç¼©æ”¾å‚æ•°ï¼ˆtrainable rescaling parametersï¼‰å°†æ•°æ®æ”¾åœ¨ä¸€ä¸ªæ–°çš„å°ºåº¦ä¸Šã€‚

ä½¿ç”¨batchnormçš„æ¨¡å‹å¾€å¾€éœ€è¦è¾ƒå°‘çš„æ—¶é—´å®Œæˆè®­ç»ƒï¼Œä¹Ÿå¯ä»¥è§£å†³å¯èƒ½å¯¼è‡´è®­ç»ƒâ€œåœæ»â€çš„å„ç§é—®é¢˜ï¼Œæ‰€ä»¥å¯ä»¥è€ƒè™‘åœ¨æ¨¡å‹ä¸­æ·»åŠ batchnormã€‚

### Adding Batch Normalization

batchnormå¯ä»¥æ”¾åœ¨ç›¸å¯¹å…¶ä»–å±‚çš„å„ç§ä½ç½®ä¸Šï¼Œå¦‚æœç”¨å®ƒä½œä¸ºç½‘ç»œçš„ç¬¬ä¸€å±‚ï¼Œå°±èµ·åˆ°äº†ä¸€ä¸ªä»£æ›¿é¢„å¤„ç†æ—¶å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–çš„æ“ä½œï¼Œç±»ä¼¼Sci-Kit Learnçš„ `StandardScaler`ï¼Œä¹Ÿå¯ä»¥æ”¾åœ¨æŸä¸€å±‚ä¹‹åï¼š
```python
layers.Dense(16, activation='relu'),
layers.BatchNormalization(),
```
æˆ–åœ¨æŸå±‚å’Œå®ƒçš„æ¿€æ´»å‡½æ•°ä¹‹é—´ï¼š
```python
layers.Dense(16),
layers.BatchNormalization(),
layers.Activation('relu'),
```

### Example - Using Dropout and Batch Normalization

å¦‚æœæ¨¡å‹ä¸­ä½¿ç”¨äº†dropoutï¼Œå°±åº”è¯¥åœ¨å±‚ä¸­æ·»åŠ æ›´å¤šçš„å•å…ƒï¼Œå› ä¸ºæ¯æ¬¡éƒ½ä¼šè¢«éšæœºæŠ›å¼ƒä¸€éƒ¨åˆ†ä¸å‚ä¸çš„å•å…ƒã€‚

```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(1024, activation='relu', input_shape=[11]),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1024, activation='relu'),
    layers.Dropout(0.3),
    layers.BatchNormalization(),
    layers.Dense(1),
])

model.compile(
    optimizer='adam',
    loss='mae',
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=256,
    epochs=100,
    verbose=0,
)

# Show the learning curves
history_df = pd.DataFrame(history.history)
history_df.loc[:, ['loss', 'val_loss']].plot();
```

## Binary Classification

### Introduction

ä¹‹å‰çš„éƒ¨åˆ†åœ¨ä»‹ç»ç”¨æ·±åº¦å­¦ä¹ è§£å†³å›å½’é—®é¢˜ï¼Œè¿™ä¸€èŠ‚ä»‹ç»ç”¨æ·±åº¦å­¦ä¹ è§£å†³åˆ†ç±»é—®é¢˜ã€‚

### Binary Classification

äºŒåˆ†ç±»é—®é¢˜æ˜¯æŒ‡åˆ†æˆä¸¤ç±»çš„é—®é¢˜ï¼Œæ¯”å¦‚ç”¨"Yes"/"No"æ¥å›ç­”çš„é—®é¢˜ã€‚æˆ‘ä»¬éœ€è¦ç»™æ•°æ®**class label**ï¼š0 æˆ– 1ï¼Œæ•°å­—æ ‡ç­¾æ˜¯ç¥ç»ç½‘ç»œæ¨¡å‹å¯ä»¥ä½¿ç”¨çš„æ•°æ®å½¢å¼ã€‚

### Accuracy and Cross-Entropy

**å‡†ç¡®æ€§**ï¼ˆAccuracyï¼‰æ˜¯è¡¡é‡åˆ†ç±»é—®é¢˜æˆåŠŸä¸å¦çš„ä¼—å¤šæŒ‡æ ‡ä¹‹ä¸€ã€‚accuracyæ˜¯æ­£ç¡®é¢„æµ‹ä¸æ€»é¢„æµ‹çš„æ¯”ç‡ï¼š`accuracy = number_correct / total`ã€‚ä¸€ä¸ªæ€»æ˜¯æ­£ç¡®é¢„æµ‹çš„æ¨¡å‹çš„å‡†ç¡®åº¦å¾—åˆ†ä¸º`1.0`ã€‚åœ¨æ‰€æœ‰å…¶ä»–æ¡ä»¶ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œæ¯å½“æ•°æ®é›†ä¸­çš„ç±»ä»¥å¤§çº¦ç›¸åŒçš„é¢‘ç‡å‡ºç°æ—¶ï¼Œå‡†ç¡®åº¦æ˜¯ä¸€ä¸ªåˆç†çš„æŒ‡æ ‡ã€‚

accuracyï¼ˆä»¥åŠå¤§å¤šæ•°å…¶ä»–åˆ†ç±»æŒ‡æ ‡ï¼‰çš„é—®é¢˜åœ¨äºï¼Œå®ƒä¸èƒ½ç”¨ä½œæŸå¤±å‡½æ•°ã€‚SGDéœ€è¦ä¸€ä¸ªå¹³ç¨³å˜åŒ–çš„æŸå¤±å‡½æ•°ï¼Œä½†ç²¾åº¦ï¼Œä½œä¸ºè®¡æ•°çš„æ¯”ç‡ï¼Œåœ¨â€œè·³è·ƒâ€ä¸­å˜åŒ–ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»é€‰æ‹©ä¸€ä¸ªæ›¿ä»£å“ä½œä¸ºæŸå¤±å‡½æ•°ã€‚è¿™ä¸ªæ›¿ä»£å“æ˜¯äº¤å‰ç†µå‡½æ•°ï¼ˆcross-entropy functionï¼‰ã€‚

ç°åœ¨ï¼Œå›æƒ³ä¸€ä¸‹æŸå¤±å‡½æ•°å®šä¹‰äº†è®­ç»ƒæœŸé—´ç½‘ç»œçš„ç›®æ ‡ã€‚é€šè¿‡å›å½’ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å°åŒ–é¢„æœŸç»“æœå’Œé¢„æµ‹ç»“æœä¹‹é—´çš„è·ç¦»ã€‚æˆ‘ä»¬é€‰æ‹©äº†MAEæ¥æµ‹é‡è¿™ä¸ªè·ç¦»ã€‚

å¯¹äºåˆ†ç±»ï¼Œæˆ‘ä»¬æƒ³è¦çš„æ˜¯æ¦‚ç‡ä¹‹é—´çš„è·ç¦»ï¼Œè¿™å°±æ˜¯äº¤å‰ç†µæä¾›çš„ã€‚**Cross-entropy**æ˜¯ä¸€ç§åº¦é‡ä»ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒåˆ°å¦ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„è·ç¦»çš„æ–¹æ³•ã€‚


![Cross-entropy penalizes incorrect probability predictions.](/blog/Notes-Kaggle-Courses-Intro-to-DL/016.png)

æˆ‘ä»¬å¸Œæœ›æˆ‘ä»¬çš„ç½‘ç»œä»¥`1.0`çš„æ¦‚ç‡é¢„æµ‹æ­£ç¡®çš„ç­çº§ã€‚é¢„æµ‹æ¦‚ç‡ç¦»`1.0`è¶Šè¿œï¼Œäº¤å‰ç†µæŸå¤±è¶Šå¤§ã€‚

æˆ‘ä»¬ä½¿ç”¨äº¤å‰ç†µçš„æŠ€æœ¯åŸå› æœ‰ç‚¹å¾®å¦™ï¼Œä½†ä»è¿™ä¸€èŠ‚ä¸­æˆ‘ä»¬è¦äº†è§£çš„ä¸»è¦å†…å®¹æ˜¯ï¼šä½¿ç”¨äº¤å‰ç†µæ¥è¿›è¡Œåˆ†ç±»æŸå¤±ï¼›ä½ å¯èƒ½å…³å¿ƒçš„å…¶ä»–æŒ‡æ ‡ï¼ˆå¦‚å‡†ç¡®æ€§ï¼‰ä¹Ÿä¼šéšä¹‹æé«˜ã€‚

### Making Probabilities with the Sigmoid Function

äº¤å‰ç†µå’Œç²¾åº¦å‡½æ•°éƒ½éœ€è¦æ¦‚ç‡ä½œä¸ºè¾“å…¥ï¼Œå³0åˆ°1ä¹‹é—´çš„æ•°å­—ã€‚ä¸ºäº†å°†å¯†é›†å±‚äº§ç”Ÿçš„å®å€¼è¾“å‡ºè½¬åŒ–ä¸ºæ¦‚ç‡ï¼Œæˆ‘ä»¬é™„åŠ äº†ä¸€ç§æ–°çš„æ¿€æ´»å‡½æ•°ï¼Œå³sigmoidæ¿€æ´»å‡½æ•°ã€‚

![The sigmoid function maps real numbers into the interval [0,1].](/blog/Notes-Kaggle-Courses-Intro-to-DL/017.png)

ä¸ºäº†å¾—åˆ°æœ€ç»ˆçš„ç±»é¢„æµ‹ï¼Œæˆ‘ä»¬å®šä¹‰äº†ä¸€ä¸ªé˜ˆå€¼æ¦‚ç‡ã€‚é€šå¸¸è¿™å°†æ˜¯0.5ï¼Œå› æ­¤å››èˆäº”å…¥å°†ä¸ºæˆ‘ä»¬æä¾›æ­£ç¡®çš„ç±»åˆ«ï¼šä½äº0.5è¡¨ç¤ºæ ‡ç­¾ä¸º0çš„ç±»åˆ«ï¼Œ0.5æˆ–ä»¥ä¸Šè¡¨ç¤ºæ ‡ç­¾ä¸º1çš„ç±»åˆ«ã€‚0.5é˜ˆå€¼æ˜¯Kerasé»˜è®¤ä½¿ç”¨çš„ç²¾åº¦æŒ‡æ ‡ã€‚

### Example - Binary Classification

é™¤äº†æœ€åä¸€å±‚ç”¨äº†â€œsigmoidâ€æ¿€æ´»ï¼Œå®ƒç”¨æ¥äº§ç”Ÿç±»æ¦‚ç‡ï¼Œå…¶ä»–éƒ¨åˆ†å’Œå›å½’ä»»åŠ¡ä¸€æ ·ã€‚

```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Dense(4, activation='relu', input_shape=[33]),
    layers.Dense(4, activation='relu'),    
    layers.Dense(1, activation='sigmoid'),
])

model.compile(
    optimizer='adam',              # Adamä¹Ÿé€‚ç”¨äºåˆ†ç±»é—®é¢˜
    loss='binary_crossentropy',    # æŸå¤±å‡½æ•°ä¸º äº¤å‰ç†µå‡½æ•°
    metrics=['binary_accuracy'],
)

# æå‰åœæ­¢ å›è°ƒå‡½æ•°
early_stopping = keras.callbacks.EarlyStopping(
    patience=10,
    min_delta=0.001,
    restore_best_weights=True,
)

history = model.fit(
    X_train, y_train,
    validation_data=(X_valid, y_valid),
    batch_size=512,
    epochs=1000,
    callbacks=[early_stopping],
    verbose=0, # hide the output because we have so many epochs
)

history_df = pd.DataFrame(history.history)
# Start the plot at epoch 5
history_df.loc[5:, ['loss', 'val_loss']].plot()
history_df.loc[5:, ['binary_accuracy', 'val_binary_accuracy']].plot()

print(("Best Validation Loss: {:0.4f}" +\
      "\nBest Validation Accuracy: {:0.4f}")\
      .format(history_df['val_loss'].min(), 
              history_df['val_binary_accuracy'].max()))

```

```
Best Validation Loss: 0.5482
Best Validation Accuracy: 0.7619
```

<img src="/blog/Notes-Kaggle-Courses-Intro-to-DL/018.png">


<img src="/blog/Notes-Kaggle-Courses-Intro-to-DL/019.png">


## Detecting the Higgs Boson With TPUs

è¿™æ˜¯å±äºIntro to Deep Learningçš„ä¸€èŠ‚Bonus Lessonï¼Œä»‹ç»å¦‚ä½•ä½¿ç”¨TPUçš„ã€‚

```python
# TensorFlow
import tensorflow as tf
print("Tensorflow version " + tf.__version__)

# Detect and init the TPU
try: # detect TPUs
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection
    strategy = tf.distribute.TPUStrategy(tpu)
except ValueError: # detect GPUs
    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU
print("Number of accelerators: ", strategy.num_replicas_in_sync)
```

# Computer Vision

## The Convolutional Classifier

### Introduction

å·ç§¯ç¥ç»ç½‘ç»œï¼ˆconvolutional neural networksï¼‰æ˜¯æœ€æ“…é•¿ç†è§£å›¾åƒçš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºconventæˆ–CNNã€‚å·ç§¯æ˜¯ä¸€ç§æ•°å­¦è¿ç®—ï¼Œå®ƒä½¿ç½‘ç»œçš„å„å±‚å…·æœ‰ç‹¬ç‰¹çš„ç»“æ„ã€‚

### The Convolutional Classifier

ç”¨äºå›¾åƒåˆ†ç±»çš„convnetç”±ä¸¤éƒ¨åˆ†ç»„æˆï¼š**convolutional base** å’Œ **dense head**ã€‚

<img src="/blog/Notes-Kaggle-Courses-Intro-to-DL/020.png">

- Baseç”¨äº**ä»å›¾åƒä¸­æå–ç‰¹å¾**ï¼Œå®ƒä¸»è¦ç”±æ‰§è¡Œå·ç§¯è¿ç®—çš„å±‚ç»„æˆï¼Œä½†é€šå¸¸ä¹ŸåŒ…æ‹¬å…¶ä»–ç±»å‹çš„å±‚ã€‚
- Headç”¨äº**ç¡®å®šå›¾åƒçš„ç±»åˆ«**ï¼Œå®ƒä¸»è¦ç”±è‡´å¯†å±‚æ„æˆï¼Œä½†ä¹Ÿå¯èƒ½åŒ…æ‹¬å…¶ä»–å±‚ï¼Œå¦‚è„±è½å±‚ã€‚

ç‰¹å¾å¯ä»¥æ˜¯çº¿æ¡ã€é¢œè‰²ã€çº¹ç†ã€å½¢çŠ¶ã€å›¾æ¡ˆï¼Œä¹Ÿå¯ä»¥æ˜¯ä¸€äº›å¤æ‚çš„ç»„åˆã€‚

<img src="/blog/Notes-Kaggle-Courses-Intro-to-DL/021.png">

### Training the Classifier

è®­ç»ƒæœŸé—´ç½‘ç»œçš„ç›®æ ‡æ˜¯å­¦ä¹ ä¸¤ä»¶äº‹ï¼š
1. è¦ä»å›¾åƒä¸­æå–å“ªäº›ç‰¹å¾ (base)ï¼Œ
2. å“ªä¸€ç±»ä¸å“ªäº›ç‰¹å¾ç›¸åŒ¹é… (head)ã€‚

å¦‚ä»Šï¼Œconvnetå¾ˆå°‘ä»é›¶å¼€å§‹è®­ç»ƒã€‚æ›´å¸¸è§çš„æƒ…å†µæ˜¯ï¼Œæˆ‘ä»¬é‡ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„åŸºç¡€ã€‚ç„¶åï¼Œæˆ‘ä»¬åœ¨é¢„å…ˆè®­ç»ƒå¥½çš„BaseåŠ ä¸Šä¸€ä¸ªæœªç»è®­ç»ƒçš„Headã€‚æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬é‡ç”¨ç½‘ç»œä¸­å·²ç»å­¦ä¼šåš*æå–ç‰¹å¾*çš„å±‚ï¼Œå¹¶é™„åŠ ä¸€äº›æ–°çš„å±‚æ¥å­¦ä¹ *åˆ†ç±»*ã€‚

<div align="center"><img style="width:50%" src="/blog/Notes-Kaggle-Courses-Intro-to-DL/022.png"></div>

å› ä¸ºå¤´éƒ¨é€šå¸¸åªæœ‰å‡ ä¸ªå¯†é›†çš„å±‚ï¼Œæ‰€ä»¥å¯ä»¥ä»ç›¸å¯¹è¾ƒå°‘çš„æ•°æ®ä¸­åˆ›å»ºéå¸¸ç²¾ç¡®çš„åˆ†ç±»å™¨ã€‚

**è¿ç§»å­¦ä¹ **å°±æ˜¯ä¸€ç§é‡ç”¨é¢„å…ˆè®­ç»ƒå¥½çš„æ¨¡å‹çš„æŠ€æœ¯ã€‚å®ƒéå¸¸æœ‰æ•ˆï¼Œç°åœ¨å‡ ä¹æ‰€æœ‰çš„å›¾åƒåˆ†ç±»å™¨éƒ½ä¼šä½¿ç”¨å®ƒã€‚


### Example - Train a Convnet Classifier

æˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªç”¨äºåˆ†ç±»æ±½è½¦å’Œå¡è½¦çš„åˆ†ç±»å™¨ï¼Œæ•°æ®é›†æ˜¯å¤§çº¦10000å¼ å›¾ç‰‡ï¼Œå…¶ä¸­æ±½è½¦å’Œå¡è½¦çš„å‡ ä¹å„å ä¸€åŠã€‚

#### Step 1 - Load Data

```python
# å¯¼å…¥ä¸€äº›åŒ…
import os, warnings
import matplotlib.pyplot as plt
from matplotlib import gridspec

import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing import image_dataset_from_directory

# è®¾ç½®å›ºå®šçš„ç§å­ï¼Œæ¥ä¿è¯å¯å¤ç°æ€§
def set_seed(seed=31415):
    np.random.seed(seed)
    tf.random.set_seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    os.environ['TF_DETERMINISTIC_OPS'] = '1'
set_seed(31415)

# è®¾ç½® Matplotlib çš„é»˜è®¤å€¼
plt.rc('figure', autolayout=True)
plt.rc('axes', labelweight='bold', labelsize='large',
       titleweight='bold', titlesize=18, titlepad=10)
plt.rc('image', cmap='magma')
warnings.filterwarnings("ignore") # to clean up output cells

# Load training and validation sets
ds_train_ = image_dataset_from_directory(
    '../input/car-or-truck/train',  # æ–‡ä»¶å¤¹
    labels='inferred',
    label_mode='binary',
    image_size=[128, 128],          # å›¾ç‰‡å¤§å°
    interpolation='nearest',
    batch_size=64,
    shuffle=True,
)
ds_valid_ = image_dataset_from_directory(
    '../input/car-or-truck/valid',
    labels='inferred',
    label_mode='binary',
    image_size=[128, 128],
    interpolation='nearest',
    batch_size=64,
    shuffle=False,
)

# Data Pipeline
def convert_to_float(image, label):
    image = tf.image.convert_image_dtype(image, dtype=tf.float32)
    return image, label

AUTOTUNE = tf.data.experimental.AUTOTUNE
ds_train = (
    ds_train_
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)
ds_valid = (
    ds_valid_
    .map(convert_to_float)
    .cache()
    .prefetch(buffer_size=AUTOTUNE)
)
```

#### Step 2 - Define Pretrained Base

æœ€å¸¸ç”¨çš„é¢„è®­ç»ƒæ•°æ®é›†æ˜¯[ImageNet](http://image-net.org/about-overview)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šç§è‡ªç„¶å›¾åƒçš„å¤§å‹æ•°æ®é›†ã€‚Kerasåœ¨å…¶ `applications`æ¨¡å—ä¸­åŒ…å«åœ¨ImageNetä¸Šé¢„è®­ç»ƒçš„å„ç§æ¨¡å‹ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹ç§°ä¸º**VGG16**ã€‚

```python
pretrained_base = tf.keras.models.load_model(
    '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',
)
pretrained_base.trainable = False
```

[VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16)ä¹Ÿå¯ä»¥è¿™æ ·ç›´æ¥è°ƒç”¨ï¼š
```python
tf.keras.applications.vgg16.VGG16(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
```

#### Step 3 - Attach Head

æ¥ä¸‹æ¥è¿æ¥åˆ†ç±»å™¨headã€‚å…ˆç”¨ä¸€ä¸ª`Flatten`å±‚ï¼ŒæŠŠå‰é¢äºŒç»´çš„è¾“å‡ºè½¬åŒ–ä¸ºä¸€ç»´æ¥æä¾›ç»™åé¢çš„å±‚ã€‚ç„¶åæ˜¯ä¸€ä¸ªéšè—å±‚ï¼Œæœ€åä¸€å±‚ï¼ˆè¾“å‡ºå±‚ï¼‰æŠŠè¾“å‡ºè½¬æ¢ä¸ºåˆ¤æ–­æ˜¯`Truck`çš„æ¦‚ç‡åˆ†æ•°ã€‚

```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    pretrained_base,
    layers.Flatten(),
    layers.Dense(6, activation='relu'),
    layers.Dense(1, activation='sigmoid'),
])
```

#### Step 4 - Train

ç”±äºè¿™æ˜¯ä¸€ä¸ªåˆ†æˆä¸¤ç±»çš„é—®é¢˜ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨äºŒè¿›åˆ¶ç‰ˆæœ¬çš„`crossentropy`å’Œ`accuracy`ã€‚

```python
model.compile(
    optimizer='adam',
    loss='binary_crossentropy',   # æŸå¤±å‡½æ•°
    metrics=['binary_accuracy'],  # å‡†ç¡®åº¦è¯„ä¼°
)

history = model.fit(
    ds_train,
    validation_data=ds_valid,
    epochs=30,
    verbose=0,
)
```

åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ¨¡å‹æ—¶ï¼Œæœ€å¥½æ£€æŸ¥losså’Œmetricæ›²çº¿ï¼Œå˜åŒ–è¿‡ç¨‹è¢«å­˜å‚¨åœ¨`history.history`ä¸­ï¼Œå¯ä»¥è¿™æ ·æŠŠå®ƒä»¬æ˜¾ç¤ºå‡ºæ¥ï¼š

```python
import pandas as pd

history_frame = pd.DataFrame(history.history)
history_frame.loc[:, ['loss', 'val_loss']].plot()
history_frame.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();
```
<img src="/blog/Notes-Kaggle-Courses-Intro-to-DL/023.png">

<img src="/blog/Notes-Kaggle-Courses-Intro-to-DL/024.png">

## Convolution and ReLU

å‰ä¸€èŠ‚æˆ‘ä»¬äº†è§£åˆ°å·ç§¯åˆ†ç±»å™¨æœ‰ä¸¤éƒ¨åˆ†ï¼šbaseå’Œheadï¼Œbaseä»å›¾åƒä¸­æå–ç‰¹å¾ï¼Œheadä½¿ç”¨è¿™äº›ç‰¹å¾å¯¹å›¾åƒåˆ†ç±»ã€‚


åé¢çš„å‡ èŠ‚æ•™ç¨‹ä¼šä»‹ç»baseéƒ¨åˆ† æœ€é‡è¦çš„ä¸¤ç§ç±»å‹çš„å±‚ï¼Œå®ƒä»¬åˆ†åˆ«æ˜¯ï¼šå…·æœ‰ReLUæ¿€æ´»çš„å·ç§¯å±‚ å’Œ æœ€å¤§æ± åŒ–å±‚ã€‚ç¬¬5èŠ‚æ•™ç¨‹å°†ä¼šä»‹ç»é€šè¿‡è¿™äº›å±‚çš„ç»„åˆæ¥è¿›è¡Œç‰¹å¾æå–ï¼ˆBaseéƒ¨åˆ†ï¼‰ã€‚

è¿™ä¸€èŠ‚æ˜¯å…³äºå·ç§¯å±‚å’ŒReLUæ¿€æ´»å‡½æ•°çš„ã€‚

### Feature Extraction

ç”±baseæ‰§è¡Œçš„ç‰¹å¾æå–åŒ…æ‹¬ä¸‰ä¸ªåŸºæœ¬æ“ä½œï¼š

1. é’ˆå¯¹ç‰¹å®šç‰¹å¾**è¿‡æ»¤**ï¼ˆfilterï¼‰å›¾åƒï¼ˆå·ç§¯ï¼‰
2. åœ¨è¿‡æ»¤åçš„å›¾åƒä¸­**æ£€æµ‹**ï¼ˆdetectï¼‰è¯¥ç‰¹å¾ï¼ˆReLUï¼‰
3. **å‹ç¼©**ï¼ˆcondenseï¼‰å›¾åƒä»¥å¢å¼ºç‰¹å¾ï¼ˆæœ€å¤§æ± åŒ–ï¼‰

ä¸‹å›¾è¯´æ˜äº†è¿™ä¸ªè¿‡ç¨‹ï¼Œå¯ä»¥çœ‹åˆ°è¿™ä¸‰ä¸ªæ“ä½œæ˜¯å¦‚ä½•éš”ç¦»åŸå§‹å›¾åƒçš„æŸäº›ç‰¹å®šç‰¹å¾çš„ï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸ºæ°´å¹³çº¿ï¼‰ã€‚

<div align="center"><img style="width:50%" src="/blog/Notes-Kaggle-Courses-Intro-to-DL/025.jpg"></div>

é€šå¸¸ç½‘ç»œä¼šåœ¨ä¸€ä¸ªå›¾ç‰‡ä¸Šå¹¶è¡Œçš„æå–ï¼Œåœ¨ä¸€äº›ç°ä»£çš„convnetsä¸­ï¼Œæœ€åä¸€å±‚äº§ç”Ÿ1000å¤šä¸ªç‹¬ç‰¹çš„è§†è§‰ç‰¹å¾ä¹Ÿå¾ˆå¤šè§ã€‚

### Filter with Convolution

å·ç§¯å±‚æ‰§è¡Œæ»¤æ³¢æ­¥éª¤,å¯ä»¥åœ¨Kerasæ¨¡å‹ä¸­å®šä¹‰ä¸€ä¸ªå·ç§¯å±‚ï¼š
```python
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential([
    layers.Conv2D(filters=64, kernel_size=3), # activation is None
    # More layers follow
])
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡è§‚å¯Ÿè¿™äº›å‚æ•°ä¸å±‚çš„æƒé‡å’Œæ¿€æ´»çš„å…³ç³»æ¥ç†è§£è¿™äº›å‚æ•°ã€‚

#### Weights

convnetåœ¨è®­ç»ƒæœŸé—´å­¦ä¹ çš„æƒé‡ä¸»è¦åŒ…å«åœ¨å…¶å·ç§¯å±‚ä¸­ï¼Œè¿™äº›**æƒé‡**æˆ‘ä»¬ç§°ä¹‹ä¸ºæ ¸ï¼ˆkernelsï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬è¡¨ç¤ºä¸ºå°æ•°ç»„ï¼š

<div align="center"><img style="width:20%" src="/blog/Notes-Kaggle-Courses-Intro-to-DL/026.png"></div>

**kernel**é€šè¿‡æ‰«æå›¾åƒå¹¶äº§ç”Ÿåƒç´ å€¼çš„åŠ æƒå’Œæ¥è¿è¡Œã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå†…æ ¸å°†åƒåæŒ¯å…‰é€é•œä¸€æ ·ï¼Œå¼ºè°ƒæˆ–ä¸å¼ºè°ƒæŸäº›ä¿¡æ¯æ¨¡å¼ã€‚


<div align="center"><img style="width:30%" src="/blog/Notes-Kaggle-Courses-Intro-to-DL/027.png"></div>

Kernelså®šä¹‰äº†å·ç§¯å±‚å¦‚ä½•è¿æ¥åˆ°åé¢çš„å±‚ï¼Œä¸Šå›¾ä¸­çš„kernelå°†å‰ä¸€å±‚çš„9ä¸ªç¥ç»å…ƒçš„è¾“å‡ºï¼ŒåŠ æƒæ±‚å’Œå¾—åˆ°ä¸€ä¸ªå€¼è¾“å…¥åˆ°äº†åé¢å±‚çš„ä¸€ä¸ªç¥ç»å…ƒã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨`kernel_size`æ¥è®¾ç½®kernelçš„ç»´åº¦ï¼Œå¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œkernelçš„ç»´æ•°éƒ½æ˜¯å¥‡æ•°ï¼Œå¦‚`(3, 3)`ï¼Œ`(5, 5)`ï¼Œå› æ­¤åªæœ‰ä¸€ä¸ªåƒç´ ä½äºä¸­å¿ƒï¼Œä½†è¿™å¹¶ä¸æ˜¯å¿…é¡»çš„ã€‚

å·ç§¯å±‚çš„kernelå†³å®šäº†å®ƒåˆ›å»ºçš„ç‰¹å¾ç±»å‹ï¼Œåœ¨è®­ç»ƒæœŸé—´ï¼Œconventä¼šå°è¯•è§£å†³å½“å‰åˆ†ç±»é—®é¢˜æ‰€éœ€è¦çš„ç‰¹å¾ï¼Œè¿™ä¹Ÿæ„å‘³ç€kernelçš„æœ€ä½³å–å€¼ã€‚

### Activations

ç½‘ç»œä¸­çš„æ¿€æ´»ï¼ˆactivationï¼‰ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºç‰¹å¾æ˜ å°„ï¼ˆfeature mapsï¼‰ï¼Œå®ƒä»¬æ˜¯æˆ‘ä»¬å¯¹å›¾åƒåº”ç”¨è¿‡æ»¤å™¨æ—¶çš„ç»“æœï¼›å®ƒä»¬åŒ…å«kernelæå–çš„è§†è§‰ç‰¹å¾ã€‚ä¸‹é¢æ˜¯ä¸€äº›kernelåŠå…¶ç”Ÿæˆçš„ç‰¹å¾æ˜ å°„ï¼š

<div align="center"><img style="width:80%" src="/blog/Notes-Kaggle-Courses-Intro-to-DL/028.png"></div>

ä»kernelä¸­çš„æ•°å­—çš„æ¨¡å¼ï¼Œå¯ä»¥çœ‹å‡ºå®ƒåˆ›å»ºçš„ç‰¹å¾æ˜ å°„çš„ç±»å‹ã€‚é€šå¸¸ï¼Œå·ç§¯åœ¨å…¶è¾“å…¥ä¸­å¼ºè°ƒçš„å†…å®¹å°†ä¸å†…æ ¸ä¸­æ­£æ•°çš„å½¢çŠ¶ç›¸åŒ¹é…ã€‚ä¸Šé¢çš„å·¦æ ¸å’Œä¸­æ ¸éƒ½å°†è¿‡æ»¤æ°´å¹³å½¢çŠ¶ã€‚

ä½¿ç”¨filterså‚æ•°ï¼Œå¯ä»¥å‘Šè¯‰å·ç§¯å±‚å¸Œæœ›å®ƒåˆ›å»ºå¤šå°‘ä¸ªç‰¹å¾è´´å›¾ä½œä¸ºè¾“å‡ºã€‚

<!-- ### Detect with ReLU

<div align="center"><img style="width:40%" src="https://s1.ax1x.com/2022/03/27/qBmK7q.png"></div>


![](https://s1.ax1x.com/2022/03/27/qBmE9S.png)
![](https://s1.ax1x.com/2022/03/27/qBmkh8.png)
![](https://s1.ax1x.com/2022/03/27/qBmZcQ.png)
![](https://s1.ax1x.com/2022/03/27/qBmV1g.png)
![](https://s1.ax1x.com/2022/03/27/qBmFtf.png)
![](https://s1.ax1x.com/2022/03/27/qBmeXj.png)
![](https://s1.ax1x.com/2022/03/27/qBmuBn.png)
![](https://s1.ax1x.com/2022/03/27/qBmnns.png)
![](https://s1.ax1x.com/2022/03/27/qBmQA0.png)
![](https://s1.ax1x.com/2022/03/27/qBmlNV.png) -->

